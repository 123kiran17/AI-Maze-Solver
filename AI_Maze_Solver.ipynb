{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPaCMR5BqIme",
        "outputId": "bd73042b-c45e-4e29-f05b-e4b6b82e9dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "import os, sys, time, datetime, json, random\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import ELU, PReLU, LeakyReLU\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fvScOxGzejGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experience "
      ],
      "metadata": {
        "id": "YqCLQdJQdw96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model, max_memory=100, discount=0.95):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.num_actions = model.output_shape[-1]\n",
        "\n",
        "    def remember(self, episode):\n",
        "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
        "        # memory[i] = episode\n",
        "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
        "        self.memory.append(episode)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, envstate):\n",
        "        return self.model.predict(envstate)[0]\n",
        "\n",
        "    def get_data(self, data_size=10):\n",
        "        env_size = self.memory[0][0].shape[1]  # envstate 1d size (1st element of episode)\n",
        "        mem_size = len(self.memory)\n",
        "        data_size = min(mem_size, data_size)\n",
        "        inputs = np.zeros((data_size, env_size))\n",
        "        targets = np.zeros((data_size, self.num_actions))\n",
        "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
        "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
        "            inputs[i] = envstate\n",
        "            # There should be no target values for actions not taken.\n",
        "            targets[i] = self.predict(envstate)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(envstate_next))\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ],
      "metadata": {
        "id": "ezF5djqaqX6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
        "rat_mark = 0.5  # The current rat cell will be painteg by gray 0.5\n",
        "\n",
        "LEFT = 0\n",
        "UP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict = {\n",
        "    LEFT: 'left',\n",
        "    UP: 'up',\n",
        "    RIGHT: 'right',\n",
        "    DOWN: 'down',\n",
        "}\n",
        "\n",
        "actions_movement = {\n",
        "    LEFT: (0, -1),\n",
        "    UP: (-1, 0),\n",
        "    RIGHT: (0, 1),\n",
        "    DOWN: (1, 0),\n",
        "}\n",
        "\n",
        "\n",
        "class Qmaze(object):\n",
        "    def __init__(self, maze, rat=(0, 0)):\n",
        "        self._maze = np.array(maze)\n",
        "        self.num_actions = len(actions_dict)\n",
        "        self.row_size, self.col_size = self._maze.shape\n",
        "        self.target = (self.row_size - 1, self.col_size - 1)  # target cell where the \"cheese\" is\n",
        "        self.free_cells = [(r, c) for r in range(self.row_size) for c in range(self.col_size) if\n",
        "                           self._maze[r, c] == 1.0]\n",
        "        self.free_cells.remove(self.target)\n",
        "        if self._maze[self.target] == 0.0:\n",
        "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
        "        if not rat in self.free_cells:\n",
        "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
        "        self.reset(rat)\n",
        "\n",
        "    def reset(self, rat):\n",
        "        self.rat = rat\n",
        "        self.maze = np.copy(self._maze)\n",
        "        row, col = rat\n",
        "        self.maze[row, col] = rat_mark\n",
        "        self.state = (row, col, 'start')\n",
        "        self.min_reward = -0.5 * self.maze.size\n",
        "        self.total_reward = 0\n",
        "        self.visited = set()\n",
        "\n",
        "    def update_state(self, action):\n",
        "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
        "\n",
        "        if self.maze[rat_row, rat_col] > 0.0:\n",
        "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
        "\n",
        "        valid_actions = self.valid_actions()\n",
        "\n",
        "        if not valid_actions:\n",
        "            nmode = 'blocked'\n",
        "        elif action in valid_actions:\n",
        "            nmode = 'valid'\n",
        "            if action == LEFT:\n",
        "                ncol -= 1\n",
        "            elif action == UP:\n",
        "                nrow -= 1\n",
        "            if action == RIGHT:\n",
        "                ncol += 1\n",
        "            elif action == DOWN:\n",
        "                nrow += 1\n",
        "        else:  # invalid action, no change in rat position\n",
        "            mode = 'invalid'\n",
        "\n",
        "        # new state\n",
        "        self.state = (nrow, ncol, nmode)\n",
        "\n",
        "    def get_reward(self):\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows - 1 and rat_col == ncols - 1:\n",
        "            return 1.0\n",
        "        if mode == 'blocked':\n",
        "            return self.min_reward - 1\n",
        "        if (rat_row, rat_col) in self.visited:\n",
        "            return -0.25\n",
        "        if mode == 'invalid':\n",
        "            return -0.75\n",
        "        if mode == 'valid':\n",
        "            return -0.04\n",
        "\n",
        "    def act(self, action):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        self.total_reward += reward\n",
        "        status = self.game_status()\n",
        "        envstate = self.observe()\n",
        "        return envstate, reward, status\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.draw_env()\n",
        "        envstate = canvas.reshape((1, -1))\n",
        "        return envstate\n",
        "\n",
        "    def draw_env(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r, c] > 0.0:\n",
        "                    canvas[r, c] = 1.0\n",
        "        # draw the rat\n",
        "        row, col, valid = self.state\n",
        "        canvas[row, col] = rat_mark\n",
        "        return canvas\n",
        "\n",
        "    def game_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return 'lose'\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        if rat_row == self.row_size - 1 and rat_col == self.col_size - 1:\n",
        "            return 'win'\n",
        "\n",
        "        return 'not_over'\n",
        "\n",
        "    def valid_actions(self, cell=None):\n",
        "        if cell is None:\n",
        "            row, col, mode = self.state\n",
        "        else:\n",
        "            row, col = cell\n",
        "\n",
        "        actions = []\n",
        "        for action, (row_increase, col_increase) in actions_movement.items():\n",
        "            if self.is_valid_position(row + row_increase, col + col_increase):\n",
        "                actions.append(action)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def is_valid_position(self, row, col):\n",
        "        if 0 <= row < self.row_size and 0 <= col < self.col_size:\n",
        "            if not self.maze[row, col] == 0.0:\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "lXZN8Ynpqy3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # maze = np.array([\n",
        "    #     [1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "    #     [1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
        "    #     [1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
        "    #     [0., 0., 1., 0., 0., 1., 0., 1., 1., 1.],\n",
        "    #     [1., 1., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
        "    #     [1., 1., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
        "    #     [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "    #     [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
        "    #     [1., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
        "    #     [1., 1., 1., 1., 1., 1., 1., 0., 1., 1.]\n",
        "    # ])\n",
        "\n",
        "    maze = np.array([\n",
        "        [1., 0., 1., 1., 1., 1., 1.],\n",
        "        [1., 1., 1., 0., 0., 1., 0.],\n",
        "        [0., 0., 0., 1., 1., 1., 0.],\n",
        "        [1., 1., 1., 1., 0., 0., 1.],\n",
        "        [1., 0., 0., 0., 1., 1., 1.],\n",
        "        [1., 0., 1., 1., 1., 1., 1.],\n",
        "        [1., 1., 1., 0., 1., 1., 1.]\n",
        "    ])\n",
        "\n",
        "    qmaze = Qmaze(maze, (0, 0))\n",
        "    model = build_model(maze, qmaze.num_actions)\n",
        "\n",
        "    with open(\"/content/model.json\") as f:\n",
        "        loaded_model_json = json.load(f)\n",
        "    \n",
        "    model = model_from_json(loaded_model_json)\n",
        "    model.load_weights(\"/content/model.h5\")\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # qtrain(model, qmaze, epochs=1000, max_memory=8 * maze.size, data_size=32)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    show(qmaze)\n",
        "    play_game(model, qmaze, (0, 0))\n",
        "    show(qmaze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U2q6SKQ5jmkE",
        "outputId": "1695756b-5b71-4b13-ffa1-cba89ecb5d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 49)                2450      \n",
            "                                                                 \n",
            " p_re_lu (PReLU)             (None, 49)                49        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 49)                2450      \n",
            "                                                                 \n",
            " p_re_lu_1 (PReLU)           (None, 49)                49        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 200       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,198\n",
            "Trainable params: 5,198\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAFD0lEQVR4nO3dMU4jdxjG4f9EEYUhook0DSWS6U2LZC6zJ3DLDdwj5QT0HAAOgA9AR4GELFGaelIkRaKFBWuBb1/meSRXrPQOa/92ofq6YRga8Ov7rfoBgLcRK4QQK4QQK4QQK4QQK4T4fZs/vLOzM0wmk496lh+aTCbt4eGhZPvo6Kjt7u6WbD89Pdke0fbd3V17fHzsnvvaVrFOJpN2cnLyPk+1pfl83haLRcn2+fl5m8/nJdvX19e2R7R9fHz84tf8GAwhxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohtjpMdXh42C4vLz/qWX7o+vq6DcNQtl1ltVq109PTku3lclm6XXUcqrXWuu7ZQ26lutcC6LruW2vtW2ut9X0/u7i4+Izn+s5ms2l7e3uj216v1+3+/r5k++DgoHS77/uS7c1m025vb0u2F4tFG4bh+X8phmF482s2mw1Vrq6uRrm9XC6H1lrJq3q7ytXVVdn3/U+Sz/fnd1YIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIIVYIsVWsq9WqdV1X8hrr9mw22+p42Hu+qrf5v61OPu7v78/Ozs4+47m+U31+sGp7Op2O8tRl9Xb8ycdWeAav+vxg1fZYT11Wb1d+1gcnHyGbWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCHEVrFWnwAc43a1MZ7ZXK1WpZ+1F9+L1z4Q/z352Pf97OLi4l0/DG9VfQJwrNtVpw+rT3z2fV+yvVgs2s3Nzc+ffJzNZkOV6hOAY91uIzyzuVwuy/7O/23s2f78zgohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohYk4+rtfr0hOAY92uOn1YfeqyavtLnHysPgE41u0q1acuqzj5CF+AWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEWCGEk49vMJ1OR3l+0Pbnc/LxJ19jPT9o+/M5+QhfgFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghRMzJx7GeAKw8dXlwcND6vi/ZHuv7/SVOPo71BGDlqcvlcln2fY/1/XbyEb4AsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUKIrU4+ttamrbXbj36oF/zZWnu0bfuLb0+HYfjjuS+8Guuvouu6m2EYjm3bHuu2H4MhhFghRFKsf9m2PebtmN9ZYeyS/meFURMrhBArhBArhBArhPgb4veuUXRUDLkAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Exploration factor\n",
        "epsilon = 0.2\n",
        "\n",
        "\n",
        "def show(qmaze):\n",
        "    plt.grid('on')\n",
        "    nrows, ncols = qmaze.maze.shape\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
        "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(qmaze.maze)\n",
        "    for row, col in qmaze.visited:\n",
        "        canvas[row, col] = 0.6\n",
        "    rat_row, rat_col, _ = qmaze.state\n",
        "    canvas[rat_row, rat_col] = 0.3  # rat cell\n",
        "    # canvas[nrows - 1, ncols - 1] = 0.9  # cheese cell\n",
        "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
        "    plt.show()  # for pycharm\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def play_game(model, qmaze, rat_cell, trace=False):\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    while True:\n",
        "        prev_envstate = envstate\n",
        "        # get next action\n",
        "        q = model.predict(prev_envstate)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "        # apply action, get rewards and new state\n",
        "        envstate, reward, game_status = qmaze.act(action)\n",
        "        if trace:\n",
        "            show(qmaze)\n",
        "        if game_status == 'win':\n",
        "            return True\n",
        "        elif game_status == 'lose':\n",
        "            return False\n",
        "\n",
        "\n",
        "def completion_check(model, qmaze):\n",
        "    for cell in qmaze.free_cells:\n",
        "        if not qmaze.valid_actions(cell):\n",
        "            return False\n",
        "        if not play_game(model, qmaze, cell):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def qtrain(model, qmaze, **opt):\n",
        "    global epsilon\n",
        "    n_epoch = opt.get('n_epoch', 15000)\n",
        "    max_memory = opt.get('max_memory', 1000)\n",
        "    data_size = opt.get('data_size', 50)\n",
        "    weights_file = opt.get('weights_file', \"\")\n",
        "    name = opt.get('name', 'model')\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # If you want to continue training from a previous model,\n",
        "    # just supply the h5 file name to weights_file option\n",
        "    if weights_file:\n",
        "        print(\"loading weights from file: %s\" % (weights_file,))\n",
        "        model.load_weights(weights_file)\n",
        "\n",
        "    # Construct environment/game from numpy array: maze (see above)\n",
        "\n",
        "    # Initialize experience replay object\n",
        "    experience = Experience(model, max_memory=max_memory)\n",
        "\n",
        "    win_history = []  # history of win/lose game\n",
        "    hsize = qmaze.maze.size // 2  # history window size\n",
        "    win_rate = 0.0\n",
        "    epoch = None\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        loss = 0.0\n",
        "        rat_cell = random.choice(qmaze.free_cells)\n",
        "        qmaze.reset(rat_cell)\n",
        "        game_over = False\n",
        "\n",
        "        # get initial envstate (1d flattened canvas)\n",
        "        envstate = qmaze.observe()\n",
        "\n",
        "        n_episodes = 0\n",
        "        while not game_over:\n",
        "            valid_actions = qmaze.valid_actions()\n",
        "            if not valid_actions: break\n",
        "            prev_envstate = envstate\n",
        "            # Get next action\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = random.choice(valid_actions)\n",
        "            else:\n",
        "                action = np.argmax(experience.predict(prev_envstate))\n",
        "\n",
        "            # Apply action, get reward and new envstate\n",
        "            envstate, reward, game_status = qmaze.act(action)\n",
        "            if game_status == 'win':\n",
        "                win_history.append(1)\n",
        "                game_over = True\n",
        "            elif game_status == 'lose':\n",
        "                win_history.append(0)\n",
        "                game_over = True\n",
        "            else:\n",
        "                game_over = False\n",
        "\n",
        "            # Store episode (experience)\n",
        "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
        "            experience.remember(episode)\n",
        "            n_episodes += 1\n",
        "\n",
        "            # Train neural network model\n",
        "            inputs, targets = experience.get_data(data_size=data_size)\n",
        "            h = model.fit(\n",
        "                inputs,\n",
        "                targets,\n",
        "                epochs=8,\n",
        "                batch_size=16,\n",
        "                verbose=0,\n",
        "            )\n",
        "            loss = model.evaluate(inputs, targets, verbose=0)\n",
        "\n",
        "        if len(win_history) > hsize:\n",
        "            win_rate = sum(win_history[-hsize:]) / hsize\n",
        "\n",
        "        dt = datetime.datetime.now() - start_time\n",
        "        t = format_time(dt.total_seconds())\n",
        "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
        "        print(template.format(epoch, n_epoch - 1, loss, n_episodes, sum(win_history), win_rate, t))\n",
        "        # we simply check if training has exhausted all free cells and if in all\n",
        "        # cases the agent won\n",
        "        if win_rate > 0.9:\n",
        "            epsilon = 0.05\n",
        "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
        "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
        "            break\n",
        "\n",
        "    # Save trained model weights and architecture, this will be used by the visualization code\n",
        "    h5file = name + \".h5\"\n",
        "    json_file = name + \".json\"\n",
        "    model.save_weights(h5file, overwrite=True)\n",
        "    with open(json_file, \"w\") as outfile:\n",
        "        json.dump(model.to_json(), outfile)\n",
        "    end_time = datetime.datetime.now()\n",
        "    dt = datetime.datetime.now() - start_time\n",
        "    seconds = dt.total_seconds()\n",
        "    t = format_time(seconds)\n",
        "    print('files: %s, %s' % (h5file, json_file))\n",
        "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
        "    return seconds\n",
        "\n",
        "\n",
        "# This is a small utility for printing readable time strings:\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)\n",
        "\n",
        "\n",
        "def build_model(maze, num_actions, lr=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(maze.size))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(num_actions))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "4dSAzw5cfgnl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}